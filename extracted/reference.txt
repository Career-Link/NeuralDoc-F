{
    "References": {
        "cnn1": [
            {
                "text": "1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for image classification. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 3642\u20133649. IEEE (2012)\n\n2. Cire\u015fan, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in breast cancer histology images with deep neural networks. In: Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2013, pp. 411\u2013418. Springer (2013)\n\n3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible, high performance convolutional neural networks for image classification. In: IJCAI Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237 (2011)\n\nIntroduction to Convolutional Neural Networks         11\n\n4. Cire\u015fan, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural network committees for handwritten character classification. In: Document Analysis and Recognition (ICDAR), 2011 International Conference on. pp. 1135\u20131139. IEEE (2011)\n\n5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural networks a review. Pattern recognition 35(10), 2279\u20132301 (2002)\n\n6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware accelerated convolutional neural networks for synthetic vision systems. In: Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp. 257\u2013260. IEEE (2010)\n\n7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum 9(1), 926 (2010)\n\n8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012)\n\n9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1), 221\u2013231 (2013)\n\n10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-scale video classification with convolutional neural networks. In: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725\u20131732. IEEE (2014)\n\n11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems. pp. 1097\u20131105 (2012)\n\n12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural computation 1(4), 541\u2013551 (1989)\n\n13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278\u20132324 (1998)\n\n14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition. Neural Networks, IEEE Transactions on 9(4), 685\u2013696 (1998)\n\n15. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural networks applied to visual document analysis. In: null. p. 958. IEEE (2003)\n\n16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of Toronto (2013)\n\n17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with convolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings. IEEE. pp. 224\u2013229. IEEE (2005)\n\n18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In: Advances in Neural Information Processing Systems. pp. 2553\u20132561 (2013)\n\n19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks (siconnets) and their application of face detection. In: Neural Networks, 2003. Proceedings of the International Joint Conference on. vol. 3, pp. 2157\u20132162. IEEE (2003)\n\n20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557 (2013)\n\n21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: Computer Vision\u2013ECCV 2014, pp. 818\u2013833. Springer (2014)",
                "similarity": 1.0000001192092896
            }
        ],
        "cnn3": [
            {
                "text": "We first report the configuration which achieved the highest accuracy, followed by the search through the space of hyperparameters (using only our validation data set) leading to the optimal configuration. We close by demonstrating the ability of FF trained CNNs to implement Class Activation Maps, which is a method from the explainable AI toolbox.",
                "similarity": 0.8359439373016357
            }
        ],
        "cnn5": [
            {
                "text": "1. Cheng, Qisen and Qu, Shuhui and Lee, Janghwan. \"72-3: Deep Learning Based Visual Defect Detection in Noisy and Imbalanced Data.\" SID Symposium Digest of Technical Papers, vol. 53, no. 1, pp. 971-974, 2022.\n\n2. Cheng, Qisen and Zhang, Chang and Shen, Xiang. \"Estimation of Energy and Time Usage in 3D Printing With Multimodal Neural Network.\" 2022 4th International Conference on Frontiers Technology of Information and Computer (ICFTIC), pp. 900-903, 2022.\n\n3. Cifar10 Dataset. [online]. Avaiable:https://www.cs.toronto.edu/ kriz/cifar.html.\n\n4. Xing, Jinming. \"Enhancing Link Prediction with Fuzzy Graph Attention Networks and Dynamic Negative Sampling.\" arXiv preprint arXiv:2411.07482 (2024).\n\n5. Veli\u010dkovi\u0107, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. \"Graph attention networks.\" arXiv preprint arXiv:1710.10903 (2017).\n\n6. Hamilton, Will, Zhitao Ying, and Jure Leskovec. \"Inductive representation learning on large graphs.\" Advances in neural information processing systems 30 (2017).\n\n7. Xing, Jinming, Can Gao, and Jie Zhou. \"Weighted fuzzy rough sets-based tri-training and its application to medical diagnosis.\" Applied Soft Computing 124 (2022): 109025.\n\n8. Gao, Can, Jie Zhou, Jinming Xing, and Xiaodong Yue. \"Parameterized maximum-entropy-based three-way approximate attribute reduction.\" International Journal of Approximate Reasoning 151 (2022): 85-100.\n\n9. Xing, Jinming, Ruilin Xing, and Yan Sun. \"FGATT: A Robust Framework for Wireless Data Imputation Using Fuzzy Graph Attention Networks and Transformer Encoders.\" arXiv preprint arXiv:2412.01979 (2024).\n\n10. S. R. Livingstone and F. A. Russo, \"The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS),\" PloS one, vol. 13, no. 5, p. e0196391, 2018. Available: https://zenodo.org/record/1188976\n\n11. Xing, Jinming, Dongwen Luo, Qisen Cheng, Chang Xue, and Ruilin Xing. \"Multi-view Fuzzy Graph Attention Networks for Enhanced Graph Learning.\" arXiv preprint arXiv:2412.17271 (2024).\n\n12. G. Heigold, I. L. Moreno, S. Bengio, and N. Shazeer, \"End-to-End Text-Dependent Speaker Verification,\" in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 5115\u20135119.\n\n13. S. Hochreiter and J. Schmidhuber, \"Long Short-Term Memory,\" Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n\n14. F. A. Gers, J. Schmidhuber, and F. Cummins, \"Learning to Forget: Continual Prediction with LSTM,\" Neural Computation, vol. 12, no. 10, pp. 2451\u20132471, 2000.\n\n15. Xing, Jinming, Ruilin Xing, and Yan Sun. \"Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis Perspective.\" arXiv preprint arXiv:2411.14654 (2024).",
                "similarity": 1.0000001192092896
            }
        ],
        "pdf2": [
            {
                "text": "Abulwerdi, F. A., Xu, W., Ageeli, A. A., Yonkunas, M. J., Arun, G., Nam, H., Schneekloth Jr, J. S., Dayie, T. K., Spector, D., Baird, N., et al. Selective small-molecule targeting of a triple helix encoded by the long noncoding rna, malat1. ACS chemical biology, 14(2):223\u2013235, 2019.\n\nAdamczyk, B., Antczak, M., and Szachniuk, M. Rnasolo: a repository of cleaned pdb-derived rna 3d structures. Bioinformatics, 38(14):3668\u20133670, 2022.\n\nAlam, T., Uludag, M., Essack, M., Salhi, A., Ashoor, H., Hanks, J. B., Kapfer, C., Mineta, K., Gojobori, T., and Bajic, V. B. Farna: knowledgebase of inferred functions of non-coding rna transcripts. Nucleic acids research, 45 (5):2838\u20132848, 2017.\n\nAnand, R., Joshi, C. K., Morehead, A., Jamasb, A. R., Harris, C., Mathis, S. V., Didi, K., Hooi, B., and Lio, P. Rnaframeflow: Flow matching for de novo 3d rna backbone design. arXiv preprint arXiv:2406.13839, 2024.\n\nAshburner, M., Ball, C. A., Blake, J. A., Botstein, D., Butler, H., Cherry, J. M., Davis, A. P., Dolinski, K., Dwight, S. S., Eppig, J. T., et al. Gene ontology: tool for the unification of biology. Nature genetics, 25(1):25\u201329, 2000.\n\nBecquey, L., Angel, E., and Tahi, F. Rnanet: an automatically built dual-source dataset integrating homologous sequences and rna structures. Bioinformatics, 37(9):1218\u20131224, 2021.\n\nBoccaletto, P., Stefaniak, F., Ray, A., Cappannini, A., Mukherjee, S., Purta, E., Kurkowska, M., Shirvanizadeh, N., Destefanis, E., Groza, P., et al. Modomics: a database of rna modification pathways. 2021 update. Nucleic acids research, 50(D1):D231\u2013D235, 2022.\n\nButtenschoen, M., Morris, G. M., and Deane, C. M. Posebusters: Ai-based docking methods fail to generate physically valid poses or generalise to novel sequences. Chemical Science, 15(9):3130\u20133139, 2024.\n\nCarvajal-Patino, J. G., Mallet, V., Becerra, D., Ni\u00f1o Vasquez, L. F., Oliver, C., and Waldisp\u00fchl, J. Rnamigos2: accelerated structure-based rna virtual screening with deep graph learning. Nature Communications, 16(1):1\u201312, 2025.\n\nCech, T. R. and Steitz, J. A. The noncoding rna revolution\u2014trashing old rules to forge new ones. Cell, 157(1): 77\u201394, 2014.\n\nCorso, G., St\u00e4rk, H., Jing, B., Barzilay, R., and Jaakkola, T. Diffdock: Diffusion steps, twists, and turns for molecular docking. arXiv preprint arXiv:2210.01776, 2022.\n\nDauparas, J., Anishchenko, I., Bennett, N., Bai, H., Ragotte, R. J., Milles, L. F., Wicky, B. I., Courbet, A., de Haas, R. J., Bethel, N., et al. Robust deep learning\u2013based protein sequence design using proteinmpnn. Science, 378 (6615):49\u201356, 2022.\n\nDisney, M. D. Targeting rna with small molecules to capture opportunities at the intersection of chemistry, biology, and medicine. Journal of the American Chemical Society, 141 (17):6776\u20136790, 2019.\n\nDurairaj, J., Adeshina, Y., Cao, Z., Zhang, X., Oleinikovas, V., Duignan, T., McClure, Z., Robin, X., Kovtun, D., Rossi, E., et al. Plinder: The protein-ligand interactions dataset and evaluation resource. bioRxiv, pp. 2024\u201307, 2024.\n\nFalese, J. P., Donlic, A., and Hargrove, A. E. Targeting rna with small molecules: from fundamental principles towards the clinic. Chemical Society Reviews, 50(4): 2224\u20132243, 2021.\n\nFu, L., Niu, B., Zhu, Z., Wu, S., and Li, W. Cd-hit: accelerated for clustering the next-generation sequencing data. Bioinformatics, 28(23):3150\u20133152, 2012.\n\nGainza, P., Sverrisson, F., Monti, F., Rodol\u00e0, E., Boscaini, D., Bronstein, M., and Correia, B. Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning. Nature Methods, 17(2):184\u2013192, 2020.\n\nGligorijevic, V., Renfrew, P. D., Kosciolek, T., Leman, J. K., Berenberg, D., Vatanen, T., Chandler, C., Taylor, B. C., Fisk, I. M., Vlamakis, H., et al. Structure-based protein function prediction using graph convolutional networks. Nature communications, 12(1):3168, 2021.\n\nGlisovic, T., Bachorik, J. L., Yong, J., and Dreyfuss, G. Rna-binding proteins and post-transcriptional gene regulation. FEBS letters, 582(14):1977\u20131986, 2008.\n\nGriffiths-Jones, S., Bateman, A., Marshall, M., Khanna, A., and Eddy, S. R. Rfam: an rna family database. Nucleic acids research, 31(1):439\u2013441, 2003.\n\nHaga, C. L. and Phinney, D. G. Strategies for targeting rna with small molecule drugs. Expert Opinion on Drug Discovery, 18(2):135\u2013147, 2023.\n\nHou, J., Adhikari, B., and Cheng, J. Deepsf: deep convolutional neural network for mapping protein sequences to folds. Bioinformatics, 34(8):1295\u20131303, 2018.\n\nBenchmark for RNA 3D Structure Modeling\n\nHuang, H., Lin, Z., He, D., Hong, L., and Li, Y. Ribodiffusion: tertiary structure-based rna inverse folding with generative diffusion models. Bioinformatics, 40 (Supplement 1):i347\u2013i356, 2024.\n\nJamasb, A. R., Morehead, A., Joshi, C. K., Zhang, Z., Didi, K., Mathis, S., Harris, C., Tang, J., Cheng, J., Lio, P., et al. Evaluating representation learning on the protein structure universe. ArXiv, pp. arXiv\u20132406, 2024.\n\nJing, B., Eismann, S., Suriana, P., Townshend, R. J., and Dror, R. Learning from protein structure with geometric vector perceptrons. arXiv preprint arXiv:2009.01411, 2020.\n\nJing, B., Eismann, S., Soni, P. N., and Dror, R. O. Equivariant graph neural networks for 3d macromolecular structure, 2021. URL https://arxiv.org/abs/2106.03843.\n\nJoshi, C. K., Jamasb, A. R., Vi\u00f1as, R., Harris, C., Mathis, S. V., Morehead, A., and Lio, P. grnade: Geometric deep learning for 3d rna inverse design. bioRxiv, pp. 2024\u201303, 2024.\n\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., \u017d\u00eddek, A., Potapenko, A., et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, 2021.\n\nKouranov, A., Xie, L., de la Cruz, J., Chen, L., Westbrook, J., Bourne, P. E., and Berman, H. M. The rcsb pdb information portal for structural genomics. Nucleic acids research, 34(suppl 1):D302\u2013D305, 2006.\n\nKovtun, D., Akdel, M., Goncearenco, A., Zhou, G., Holt, G., Baugher, D., Lin, D., Adeshina, Y., Castiglione, T., Wang, X., et al. Pinder: The protein interaction dataset and evaluation resource. bioRxiv, pp. 2024\u201307, 2024.\n\nKucera, T., Oliver, C., Chen, D., and Borgwardt, K. Proteinshake: Building datasets and benchmarks for deep learning on protein structures. In Advances in Neural Information Processing Systems, volume 36, pp. 58277\u201358289, 2023.\n\nLeman, J. K., Weitzner, B. D., Lewis, S. M., Adolf-Bryfogle, J., Alam, N., Alford, R. F., Aprahamian, M., Baker, D., Barlow, K. A., Barth, P., et al. Macromolecular modeling and design in rosetta: recent methods and frameworks. Nature methods, 17(7):665\u2013680, 2020a.\n\nLeman, J. K., Weitzner, B. D., Lewis, S. M., Adolf-Bryfogle, J., Alam, N., Alford, R. F., Aprahamian, M., Baker, D., Barlow, K. A., Barth, P., et al. Macromolecular modeling and design in rosetta: recent methods and frameworks. Nature methods, 17(7):665\u2013680, 2020b.\n\nLeontis, N. B. and Zirbel, C. L. Nonredundant 3d structure datasets for rna knowledge extraction and benchmarking. RNA 3D structure analysis and prediction, pp. 281\u2013298, 2012.\n\nLorenz, R., Bernhart, S. H., H\u00f6ner zu Siederdissen, C., Tafer, H., Flamm, C., Stadler, P. F., and Hofacker, I. L. Viennarna package 2.0. Algorithms for molecular biology, 6:1\u201314, 2011.\n\nMitchell, S., OSullivan, M., and Dunning, I. Pulp: a linear programming toolkit for python. The University of Auckland, Auckland, New Zealand, 65:25, 2011.\n\nNori, D. and Jin, W. Rnaflow: Rna structure and sequence design via inverse folding-based flow matching, 2024. URL https://arxiv.org/abs/2405.18768.\n\nNotin, P., Kollasch, A., Ritter, D., van Niekerk, L., Paul, S., Spinner, H., Rollins, N., Shaw, A., Orenbuch, R., Weitzman, R., Frazer, J., Dias, M., Franceschi, D., Gal, Y., and Marks, D. Proteingym: Large-scale benchmarks for protein fitness prediction and design. In Advances in Neural Information Processing Systems, volume 36, pp. 64331\u201364379, 2023.\n\nOliver, C., Mallet, V., Gendron, R. S., Reinharz, V., Hamilton, W. L., Moitessier, N., and Waldisp\u00fchl, J. Augmented base pairing networks encode rna-small molecule binding preferences. Nucleic acids research, 48(14):7690\u20137699, 2020.\n\nOntiveros-Palacios, N., Cooke, E., Nawrocki, E. P., Triebel, S., Marz, M., Rivas, E., Griffiths-Jones, S., Petrov, A. I., Bateman, A., and Sweeney, B. Rfam 15: Rna families database in 2025. Nucleic Acids Research, 53(D1):D258\u2013D267, 2025.\n\nPanei, F. P., Torchet, R., Menager, H., Gkeka, P., and Bonomi, M. Hariboss: a curated database of rna-small molecules structures to aid rational drug design. Bioinformatics, 38(17):4185\u20134193, 2022.\n\nRen, Y., Chen, Z., Qiao, L., Jing, H., Cai, Y., Xu, S., Ye, P., Ma, X., Sun, S., Yan, H., et al. Beacon: Benchmark for comprehensive rna tasks and language models. Advances in Neural Information Processing Systems, 37:92891\u201392921, 2024.\n\nRoundtree, I. A., Evans, M. E., Pan, T., and He, C. Dynamic rna modifications in gene expression regulation. Cell, 169(7):1187\u20131200, 2017.\n\nRuiz-Carmona, S., Alvarez-Garcia, D., Foloppe, N., Garmendia-Doval, A. B., Juhos, S., Schmidtke, P., Barril, X., Hubbard, R. E., and Morley, S. D. rdock: A\n\nBenchmark for RNA 3D Structure Modeling\n\nfast, versatile and open source program for docking ligands to proteins and nucleic acids. PLoS Computational Biology, 10:1\u20138, 2014. ISSN 15537358. doi: 10.1371/journal.pcbi.1003571.\n\nSchneuing, A., Harris, C., Du, Y., Didi, K., Jamasb, A., Igashov, I., Du, W., Gomes, C., Blundell, T. L., Lio, P., et al. Structure-based drug design with equivariant diffusion models. Nature Computational Science, 4(12): 899\u2013909, 2024.\n\nStatello, L., Guo, C.-J., Chen, L.-L., and Huarte, M. Gene regulation by long non-coding rnas and its biological functions. Nature reviews Molecular cell biology, 22(2): 96\u2013118, 2021.\n\nSu, H., Peng, Z., and Yang, J. Recognition of small molecule\u2013rna binding sites using rna sequence and structure. Bioinformatics, 37(1):36\u201342, 2021.\n\nSzikszai, M., Magnus, M., Sanghi, S., Kadyan, S., Bouatta, N., and Rivas, E. Rna3db: A structurally-dissimilar dataset split for training and benchmarking deep learning models for rna structure prediction. Journal of Molecular Biology, pp. 168552, 2024. ISSN 0022-2836. doi: https://doi.org/10.1016/j.jmb.2024.168552.\n\nTan, C., Zhang, Y., Gao, Z., Hu, B., Li, S., Liu, Z., and Li, S. Z. Rdesign: hierarchical data-efficient representation learning for tertiary structure-based rna design. arXiv preprint arXiv:2301.10774, 2023.\n\nTan, C., Zhang, Y., Gao, Z., Cao, H., Li, S., Ma, S., Blanchette, M., and Li, S. Z. R3design: deep tertiary structure-based rna sequence design and beyond. Briefings in Bioinformatics, 26(1):bbae682, 2025.\n\nTownshend, R., V\u00f6gele, M., Suriana, P., Derry, A., Powers, A., Laloudakis, Y., Balachandar, S., Jing, B., Anderson, B., Eismann, S., Kondor, R., Altman, R., and Dror, R. Atom3d: Tasks on molecules in three dimensions. In Advances in Neural Information Processing Systems, Datasets and Benchmarks, volume 1, 2021a.\n\nTownshend, R. J., Eismann, S., Watkins, A. M., Rangan, R., Karelina, M., Das, R., and Dror, R. O. Geometric deep learning of rna structure. Science, 373(6558):1047\u20131051, 2021b.\n\nvan Kempen, M., Kim, S. S., Tumescheit, C., Mirdita, M., Gilchrist, C. L., S\u00f6ding, J., and Steinegger, M. Foldseek: fast and accurate protein structure search. Biorxiv, pp. 2022\u201302, 2022.\n\nVolkov, M., Turk, J.-A., Drizard, N., Martin, N., Hoffmann, B., Gaston-Math\u00e9, Y., and Rognan, D. On the frustration to predict binding affinities from protein\u2013ligand structures with deep neural networks. Journal of medicinal chemistry, 65(11):7946\u20137958, 2022.\n\nWang, J., Quan, L., Jin, Z., Wu, H., Ma, X., Wang, X., Xie, J., Pan, D., Chen, T., Wu, T., et al. Multimodrlbp: A deep learning approach for multi-modal rna-small molecule ligand binding sites prediction. IEEE Journal of Biomedical and Health Informatics, 2024.\n\nWang, K., Jian, Y., Wang, H., Zeng, C., and Zhao, Y. Rbind: computational network method to predict rna binding sites. Bioinformatics, 34(18):3131\u20133136, 2018.\n\nWang, K., Zhou, R., Wu, Y., and Li, M. Rlbind: a deep learning method to predict rna\u2013ligand binding sites. Briefings in Bioinformatics, 24(1):bbac486, 2023.\n\nWang, L., Liu, H., Liu, Y., Kurtin, J., and Ji, S. Learning hierarchical protein representations via complete 3d graph networks, 2022. URL https://arxiv.org/abs/2207.12600.\n\nWang, R., Fang, X., Lu, Y., Yang, C.-Y., and Wang, S. The pdbbind database: methodologies and updates. Journal of medicinal chemistry, 48(12):4111\u20134119, 2005a.\n\nWang, R., ueliang Fang, Lu, Y., Yang, C.-Y., and Wang, S. The pdbbind database: Methodologies and updates. Journal of Medicinal Chemistry, 22, 11 2005b. ISSN 4111\u20134119. doi: 10.1021/jm048957q.\n\nWatson, J. L., Juergens, D., Bennett, N. R., Trippe, B. L., Yim, J., Eisenach, H. E., Ahern, W., Borst, A. J., Ragotte, R. J., Milles, L. F., et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976): 1089\u20131100, 2023.\n\nWong, F., He, D., Krishnan, A., Hong, L., Wang, A. Z., Wang, J., Hu, Z., Omori, S., Li, A., Rao, J., et al. Deep generative design of rna aptamers using structural predictions. Nature Computational Science, pp. 1\u201311, 2024.\n\nXu, J., Wu, K.-j., Jia, Q.-j., and Ding, X.-f. Roles of mirna and lncrna in triple-negative breast cancer. Journal of Zhejiang University-science b, 21(9):673\u2013689, 2020.\n\nZeng, P. and Cui, Q. Rsite2: an efficient computational method to predict the functional sites of noncoding rnas. Scientific Reports, 6(1):19016, 2016.\n\nZeng, P., Li, J., Ma, W., and Cui, Q. Rsite: a computational method to identify the functional sites of noncoding rnas. Scientific Reports, 5(1):9179, 2015.\n\nZhang, C., Shine, M., Pyle, A. M., and Zhang, Y. Us-align: universal structure alignments of proteins, nucleic acids, and macromolecular complexes. Nature methods, 19(9): 1109\u20131115, 2022a.\n\nZhang, Z., Xu, M., Jamasb, A., Chenthamarakshan, V., Lozano, A., Das, P., and Tang, J. Protein representation learning by geometric structure pretraining. arXiv preprint arXiv:2203.06125, 2022b.\n\n\nZheng, J., Xie, J., Hong, X., and Liu, S. Rmalign: an\nrna structural alignment tool based on a novel scoring\nfunction rmscore. BMC genomics, 20:1\u201310, 2019.\n\nZhu, Z., Shi, C., Zhang, Z., Liu, S., Xu, M., Yuan, X.,\nZhang, Y., Chen, J., Cai, H., Lu, J., et al. Torchdrug: A\npowerful and flexible machine learning platform for drug\ndiscovery. arXiv preprint arXiv:2202.08320, 2022.",
                "similarity": 1.0000001192092896
            }
        ],
        "pdf4": [
            {
                "text": "Balasubramanian, K., Fan, J. and Yang, Z. (2018). Tensor methods for additive index models under discordance and heterogeneity. arXiv preprint arXiv:1807.06693 .\n\nBauer, B. and Kohler, M. (2019). On deep learning as a remedy for the curse of dimensionality in nonparametric regression. The Annals of Statistics 47 2261\u20132285.\n\nBauer, F., Pereverzev, S. and Rosasco, L. (2007). On regularization algorithms in learning theory. Journal of complexity 23 52\u201372.\n\nBreymann, W. and L\u00fcthi, D. (2013). ghyp: A package on generalized hyperbolic distributions. Manual for R Package ghyp .\n\nCandes, E. J., Li, X., Ma, Y. and Wright, J. (2009). Robust principal component analysis? arXiv preprint arXiv: 0912.3599 .\n\nChangliang Zou, Y. K. and Zhang, W. (2022). Estimation of low rank high-dimensional multivariate linear models for multi-response data. Journal of the American Statistical Association 117 693\u2013703.\n\nChen, K., Dong, H. and Chan, K.-S. (2012). Reduced rank regression via adaptive nuclear norm penalization. arXiv preprint arXiv:1201.0381 .\n\nChen, X., Zou, C. and Cook, R. D. (2010). Coordinate-independent sparse sufficient dimension reduction and variable selection. The Annals of Statistics 38 3696 \u2013 3723.\n\nDamian, A., Lee, J. and Soltanolkotabi, M. (2022). Neural networks can learn representations with gradient descent. In Conference on Learning Theory.\n\nFriedman, J. H. and Stuetzle, W. (1981). Projection pursuit regression. Journal of the American Statistical Association 76 817\u2013823.\n\nHui Zou, T. H. and Tibshirani, R. (2006). Sparse principal component analysis. Journal of Computational and Graphical Statistics 15 265\u2013286.\n\nHyv\u00e4rinen, A. and Dayan, P. (2005). Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research 6 695\u2013709.\n\nJanzamin, M., Sedghi, H. and Anandkumar, A. (2014). Score function features for discriminative learning: Matrix and tensor framework. arXiv preprint arXiv:1412.2863 .\n\nKobak, D., Bernaerts, Y., Weis, M. A., Scala, F., Tolias, A. S. and Berens, P. (2021). Sparse reduced-rank regression for exploratory visualisation of paired multivariate data. Journal of the Royal Statistical Society Series C: Applied Statistics 70 980\u20131000.\n\nLee, W. and Liu, Y. (2012). Simultaneous multiple response regression and inverse covariance matrix estimation via penalized gaussian maximum likelihood. Journal of Multivariate Analysis 111 241\u2013255.\n\nLi, K.-C. (1991). Sliced inverse regression for dimension reduction. Journal of the American Statistical Association 86 316\u2013327.\n\nLi, K.-C. (1992). On principal hessian directions for data visualization and dimension reduction: Another application of stein's lemma. Journal of the American Statistical Association 87 1025\u20131039.\n\nLi, K.-C. and Duan, N. (1989). Regression analysis under link violation. The Annals of Statistics 1009\u20131052.\n\nLi, Y. and Turner, R. E. (2017). Gradient estimators for implicit models. arXiv preprint arXiv:1705.07107 .\n\nLu, Z., Monteiro, R. D. and Yuan, M. (2012). Convex optimization methods for dimension reduction and coefficient estimation in multivariate linear regression. Mathematical Programming 131 163\u2013194.\n\nMakhzani, A. and Frey, B. (2013). K-sparse autoencoders. arXiv preprint arXiv:1312.5663 .\n\nMeng, C., Song, Y., Li, W. and Ermon, S. (2021). Estimating high order gradients of the data distribution by denoising. Advances in Neural Information Processing Systems 34 25359\u201325369.\n\nMousavi-Hosseini, A., Park, S., Girotti, M., Mitliagkas, I. and Erdogdu, M. A. (2022).\nNeural networks efficiently learn low-dimensional representations with sgd. arXiv preprint\narXiv:2209.14863 .\n\nMukherjee, A. and Zhu, J. (2011). Reduced rank ridge regression and its kernel extensions.\nStatistical analysis and data mining: the ASA data science journal 4 612\u2013622.\n\nO'Rourke, S., Vu, V. and Wang, K. (2018). Random perturbation of low rank matrices:\nImproving classical bounds. Linear Algebra and its Applications 540 26\u201359.\n\nPearson, K. (1901). Liii. on lines and planes of closest fit to systems of points in space. The\nLondon, Edinburgh, and Dublin philosophical magazine and journal of science 2 559\u2013572.\n\nRifai, S., Vincent, P., Muller, X., Glorot, X. and Bengio, Y. (2011). Contractive auto-\nencoders: Explicit invariance during feature extraction. In Proceedings of the 28th international\nconference on international conference on machine learning.\n\nScala, F., Kobak, D., Bernabucci, M., Bernaerts, Y., Cadwell, C. R., Castro, J. R.,\nHartmanis, L., Jiang, X., Laturnus, S., Miranda, E. et al. (2021). Phenotypic variation\nof transcriptomic cell types in mouse motor cortex. Nature 598 144\u2013150.\n\nShi, J., Sun, S. and Zhu, J. (2018). A spectral approach to gradient estimation for implicit\ndistributions. In International Conference on Machine Learning.\n\nSimon, N., Friedman, J. and Hastie, T. (2013). A blockwise descent algorithm for group-\npenalized multiresponse and multinomial regression. arXiv preprint arXiv:1311.6529 .\n\nSong, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of the data\ndistribution. Advances in neural information processing systems 32.\n\nSong, Y., Garg, S., Shi, J. and Ermon, S. (2020). Sliced score matching: A scalable approach\nto density and score estimation. In Uncertainty in Artificial Intelligence.\n\nStrathmann, H., Sejdinovic, D., Livingstone, S., Szabo, Z. and Gretton, A. (2015).\nGradient-free hamiltonian monte carlo with efficient kernel exponential families. Advances in\nNeural Information Processing Systems 28 955\u2013963.\n\nTan, K. M., Wang, Z., Zhang, T., Liu, H. and Cook, R. D. (2018). A convex formulation for\nhigh-dimensional sparse sliced inverse regression. Biometrika 105 769\u2013782.\n\nVershynin, R. (2018a). High-dimensional probability: An introduction with applications in data\nscience, vol. 47. Cambridge university press.\n\nVershynin, R. (2018b). High-dimensional probability: An introduction with applications in data\nscience, vol. 47. Cambridge university press.\n\nVincent, P. (2011). A connection between score matching and denoising autoencoders. Neural\ncomputation 23 1661\u20131674.\n\nVincent, P., Larochelle, H., Bengio, Y. and Manzagol, P.-A. (2008). Extracting and\ncomposing robust features with denoising autoencoders. In Proceedings of the 25th international\nconference on Machine learning.\n\nWANG, W., LIANG, Y. and XING, E. (2013). Block regularized lasso for multivariate multi-response linear regression. In Artificial intelligence and statistics.\n\nXU, X. (2020). On the perturbation of the moore\u2013penrose inverse of a matrix. Applied Mathematics and Computation 374 124920.\n\nYANG, Z., BALASUBRAMANIAN, K. and LIU, H. (2017a). High-dimensional non-Gaussian single index models via thresholded score function estimation. In Proceedings of the 34th International Conference on Machine Learning, vol. 70.\n\nYANG, Z., BALASUBRAMANIAN, K., WANG, Z. and LIU, H. (2017b). Learning non-gaussian multi-index model via second-order stein's method. Advances in Neural Information Processing Systems 30 6097\u20136106.\n\nYU, Y., WANG, T. and SAMWORTH, R. J. (2015). A useful variant of the davis\u2013kahan theorem for statisticians. Biometrika 102 315\u2013323.\n\nYUAN, M., EKICI, A., LU, Z. and MONTEIRO, R. (2007). Dimension reduction and coefficient estimation in multivariate linear regression. Journal of the Royal Statistical Society Series B: Statistical Methodology 69 329\u2013346.\n\nZHOU, Y., SHI, J. and ZHU, J. (2020). Nonparametric score estimators. In International Conference on Machine Learning.\n\n17\n\nSupplementary Material for\n\"Nonlinear Multiple Response Regression and Learning of Latent Spaces\"\n\nYe Tian, Sanyou Wu and Long Feng",
                "similarity": 1.0000001192092896
            }
        ],
        "cnn2": [
            {
                "text": "[1] Joakim And\u00e9n and St\u00e9phane Mallat. Deep scattering spectrum. Signal Processing, IEEE Transactions on, 62(16):4114\u20134128, 2014.\n\n[2] Joan Bruna and St\u00e9phane Mallat. Invariant scattering convolution networks. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1872\u20131886, 2013.\n\n[3] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3): 273\u2013297, 1995.\n\n[4] Joan Bruna Estrach. Scattering representations for recognition.\n\n[5] Kaiser Gerald. A friendly guide to wavelets, 1994.\n\n[6] B Boser Le Cun, John S Denker, D Henderson, Richard E Howard, W Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems. Citeseer, 1990.\n\n[7] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\n\n[8] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436\u2013444, 2015.\n\n[9] St\u00e9phane Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics, 65(10):1331\u20131398, 2012.\n\n[10] St\u00e9phane Mallat. Understanding deep convolutional networks. arXiv preprint arXiv:1601.04920, 2016.",
                "similarity": 1.0000001192092896
            }
        ],
        "cnn4": [
            {
                "text": "Convolutional Neural Networks (CNNs) have revolutionized the field of deep learning, especially in processing grid-like data structures such as images [1]. Their effectiveness in tasks like image classification [2, 3], object detection [4, 5], semantic segmentation [6, 7] and image generation [8] stem from their ability to effectively learn spatial features. Convolutional layers, using filters or kernels, capture local patterns and extract features from input images. One important feature of convolutional layers is the shared weights implemented by kernels. This allows for efficient deep learning on images, as using only fully connected layers for such tasks would result in unfathomable numbers of parameters. Pooling layers, like max pooling and average pooling, reduce the spatial dimensions of these features, helping the network to focus on the most significant aspects.\n\nDespite their popularity, CNNs face challenges in computational efficiency and adaptability. There have been several convolutional neural network architectures that have been proposed that are aimed at efficiency. Some of such architectures include MobileNet [12] and EfficientNet [13]. However, such traditional CNNs, with fixed architectures and number of parameters, may not perform uniformly across different types of input data with varying levels of complexity.\n\nNeural Architecture Search (NAS), a method for selecting optimal neural network architectures, has been a response to this challenge. NAS aims to obtain the best model for a specific task under certain constraints [14]. However, NAS is often resource-intensive due to the need to train multiple candidate models. in order to determine the optimal architecture. It is estimated that the carbon emission produced when using NAS to train a transformer model can amount to five times the lifetime carbon emissions of an average car [15]. This highlights the importance of finding suitable\n\narchitectures for neural networks, yet also points to the limitations of current approaches in terms of static structure and proneness to over-parameterization.\n\nSelf Expanding Neural Networks (SENN), introduced in [9], offer a promising direction. Inspired by neurogenesis, SENN dynamically adds neurons and fully connected layers to the architecture during training using a natural expansion score (defined in section 2.1) as a criteria to guide this process. This helps overcome the problem of over-parametrization. However, its application has been limited to multilayer perceptrons, with extensions to more practical architectures like CNNs identified as a future research prospect.\n\nOur study aims to develop a Self Expanding Convolutional Neural Network (SECNN), building on the concept of SENN and applying it to modern vision tasks. To the best of our knowledge, there has been no research on Self Expanding CNNs, despite the potential they hold for addressing model efficiency and adaptability in vision tasks. Unlike existing approaches that often require restarting training after modifications or rely on preset mechanisms for expansion, our approach utilizes the natural expansion score for dynamic and optimal model expansion. This research represents a significant step in developing adaptable, efficient CNN models for a variety of vision-related tasks.\n\nThe contributions of this research are as follows:\n\n- Developing a Self Expanding CNN that dynamically determines the optimal model size based on the task, thereby enhancing efficiency.\n- Eliminating the need to train multiple CNN models of varying sizes by allowing for the extraction of checkpoints at diverse complexity levels.\n- Eliminating the need to restart the training process after expanding the CNN model.",
                "similarity": 0.8484948873519897
            }
        ],
        "pdf1": [
            {
                "text": "[1] J. Nicholson and A. Healey, \"The present state of autonomous underwater vehicle (AUV) applications and technologies,\" Marine Technology Society Journal, vol. 42, no. 1, pp. 44\u201351, 2008.\n\n[2] G. Griffiths, Technology and applications of autonomous underwater vehicles, vol. 2. CRC Press, 2002.\n\n[3] P. A. Miller, J. A. Farrell, Y. Zhao, and V. Djapic, \"Autonomous underwater vehicle navigation,\" IEEE Journal of Oceanic Engineering, vol. 35, no. 3, pp. 663\u2013678, 2010.\n\n[4] D. Rudolph and T. A. Wilson, \"Doppler Velocity Log theory and preliminary considerations for design and construction,\" in 2012 Proceedings of IEEE Southeastcon, pp. 1\u20137, IEEE, 2012.\n\n[5] N. Cohen and I. Klein, \"Inertial navigation meets deep learning: A survey of current trends and future directions,\" Results in Engineering, p. 103565, 2024.\n\n[6] F. Zhang, S. Zhao, L. Li, and C. Cao, \"Underwater DVL Optimization Network (UDON): A Learning-Based DVL Velocity Optimizing Method for Underwater Navigation,\" Drones, vol. 9, no. 1, p. 56, 2025.\n\n[7] Liu, Peijia and Wang, Bo and Li, Guanghua and Hou, Dongdong and Zhu, Zhengyu and Wang, Zhongyong, \"Sins/dvl integrated navigation method with current compensation using rbf neural network,\" IEEE Sensors Journal, vol. 22, no. 14, pp. 14366\u201314377, 2022.\n\n[8] E. Topini, F. Fanelli, A. Topini, M. Pebody, A. Ridolfi, A. B. Phillips, and B. Allotta, \"An experimental comparison of Deep Learning strategies for AUV navigation in DVL-denied environments,\" Ocean Engineering, vol. 274, p. 114034, 2023.\n\n[9] R. Makam, M. Pramuk, S. Thomas, and S. Sundaram, \"Spectrally Normalized Memory Neuron Network Based Navigation for Autonomous Underwater Vehicles in DVL-Denied Environment,\" in OCEANS 2024-Singapore, pp. 1\u20136, IEEE, 2024.\n\n[10] Z. Yampolsky and I. Klein, \"DCNet: A data-driven framework for DVL calibration,\" Applied Ocean Research, vol. 158, p. 104525, 2025.\n\n[11] M. Yona and I. Klein, \"MissBeamNet: Learning missing Doppler velocity log beam measurements,\" Neural Computing and Applications, vol. 36, no. 9, pp. 4947\u20134958, 2024.\n\n[12] N. Cohen, Z. Yampolsky, and I. Klein, \"Set-transformer BeamsNet for AUV velocity forecasting in complete DVL outage scenarios,\" in 2023 IEEE Underwater Technology (UT), pp. 1\u20136, IEEE, 2023.\n\n[13] N. Cohen and I. Klein, \"BeamsNet: A data-driven approach enhancing Doppler velocity log measurements for autonomous underwater vehicle navigation,\" Engineering Applications of Artificial Intelligence, vol. 114, p. 105216, 2022.\n\n[14] N. Cohen and I. Klein, \"Adaptive Kalman-Informed Transformer,\" Engineering Applications of Artificial Intelligence, vol. 146, p. 110221, 2025.\n\n[15] A. Levy and I. Klein, \"Adaptive Neural Unscented Kalman Filter,\" arXiv preprint arXiv:2503.05490, 2025.\n\n[16] Y. Stolero and I. Klein, \"AUV Acceleration Prediction Using DVL and Deep Learning ,\" arXiv preprint arXiv: 2503.16573, 2025.\n\n[17] D. Simon, Optimal state estimation: Kalman, H infinity, and nonlinear approaches. John Wiley & Sons, 2006.\n\n[18] Y. Bar-Shalom, X. R. Li, and T. Kirubarajan, Estimation with applications to tracking and navigation: theory algorithms and software. John Wiley & Sons, 2004.\n\n[19] P. Groves, Principles of GNSS, Inertial, and Multisensor Integrated Navigation Systems, Second Edition. GNSS/GPS, Artech House, 2013.\n\n[20] J. Farrell, Aided navigation: GPS with high rate sensors. McGraw-Hill, Inc., 2008.",
                "similarity": 1.0000001192092896
            }
        ],
        "pdf3": [
            {
                "text": "Alford, L., \"Estimating Extreme Responses Using a Non-Uniform Phase Distribution,\" University of Michigan, Ann Arbor, MI, USA 2008.\n\nBertram, V., Practical Ship Hydrodynamics, 2nd edition, Butterworth-Heinemann ISBN 978-0080971506, 2011.\n\nBishop, R. C., Belknap, W., Turner, C., Simon, B., and Kim, J. H., \"Parametric Investigation on the Influence of GM, Roll Damping, and Above-Water Form on the\n\nRoll Response of Model 5613\", Naval Surface Warfare Center Carderock Division, West Bethesda, MD, USA, Technical Report 50-TR-2005/027, 2005.\n\nGuth, S., and Sapsis. T., \"Analytical and Computational Methods for Non-Gaussian Reliability Analysis of Nonlinear Systems Operating in Stochastic Environments,\" Massachusetts Institute of Technology, Cambridge, MA, USA 2023.\n\nHochreiter, S., and Schmidhuber, J., \"Long Short-Term Memory,\" Neural Computation, Vol. 9, No. 11, pp 1735-1780, 1997.\n\nKim, D-H., \"Design Loads Generator: Estimation of Extreme Environmental Loadings for Ship and Offshore Applications,\" University of Michigan, Ann Arbor, MI, USA 2012.\n\nLevine, M.D., Edwards, S.J., Howard, D., Weems, K., Sapsis, T., and Pipiras, V., \"Multi-Fidelity Data-Adaptive Autonomous Seakeeping,\" Ocean Engineering, Volume 292, 2024.\n\nLevine, M. D., Belenky, V., and Weems, K. M., \"Method for Automated Safe Seakeeping Guidance,\" Proceedings of the 1\u02e2\u1d57 International Conference on the Stability and Safety of Ships and Ocean Vehicles, 2021, Glasgow, UK.\n\nLin, W., and Yue, D., \"Numerical Solutions for Large Amplitude Ship Motions in the Time-Domain,\" Proceedings of the 18\u1d57\u02b0 Symposium on Naval Hydrodynamics, 1990, Ann Arbor, Michigan, USA.\n\nLonguet-Higgins, M. S., \"Statistical Properties of Wave Groups in a Random Sea State,\" Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, Vol. 312, No.1521, 1984, pp. 219-250.\n\nReed, A.M., \"Predicting Extreme Loads and the Processes for Predicting Them Efficiently,\" Proceedings of the 1\u02e2\u1d57 International Conference on the Stability and Safety of Ships and Ocean Vehicles, Glasgow, UK 2021.\n\nShin, Y.S., Belenky, V., Lin, W-M., and Weems, K.M., \"Nonlinear Time Domain Simulation Technology for Seakeeping and Wave-Load Analysis for Modern Ship Design,\" SNAME Transactions, Vol. 111, pp. 557-589, 2003.\n\nWan, Z.Y., Vlachas, P., Koumoutsakos, P., Sapsis, T., \"Data-assisted Reduced-Order Modeling of Extreme Events in Complex Dynamical Systems.\" PLOS ONE, Vol 13, Issue 5, 2018.\n\nWeems, K., and Wundrow, D., \"Hybrid Models for the Fast Time-Domain Simulation of Stability Failures in Irregular Waves with Volume based Calculations for Froude-Krylov and Hydrostatic Forces,\" 13\u1d57\u02b0 International Ship Stability Workshop, Brest, France 2013.\n\n7. DISCUSSION\n\nArmin Troesch\nABS Professor Emeritus of Marine and Design\nPerformance\nDeparment of Naval Architecture\nUniversity of Michigan\n\nThe authors are to be congratulated on presenting a well-written paper investigating the application of a neural network method in the estimation of extreme ship responses. As discussed in the paper, extensive Monte Carlo simulations containing rare extreme events are generally not feasible for complex nonlinear systems. As a result, numerous strategies, such as the one described by the authors, have been proposed and developed where ensembles of short time series containing large responses are constructed.\n\nUsing pitch maxima of an ONRT flared hull variant as an example of the method, the authors showed impressive correlation between extreme value PDF's based on Snippet LSTM and LAMP simulations. However, pitch motion is characteristic of a \"well behaved\" nonlinear process in that pitch nonlinearities become apparent in a \"regular\" fashion with increasing excitation. This allowed the authors to successfully extend the Sea State 5 results to Sea State 6 without additional training.\n\n1. Would the authors please comment (speculate?) on how they expect the multi-fidelity approach with LSTM would work when applied to nonlinear systems that contain possible bifurcations, e.g. vessel capsize.\n\nThe authors also identified potential future applications of the LSTM method.\n\n2. Do they feel that the method is applicable when the nonlinear effect of interest has no SimpleCode equivalent? For example, sonar dome slamming pressures can be estimated using CFD or an impact model in LAMP. How would the authors propose to use the LSTM method on nonlinear processes such as extreme bottom slamming loads?\n\n8. AUTHOR'S REPLY\n\nThe authors are grateful for Professor Troesch's insights and questions. The responses to the questions are below.\n\nQuestion 1: The application of neural networks to problems identifying extremes necessitates a special approach, as discussed in the paper. The basis of the solution was to make the rare events seem less rare by initially identifying candidates for large pitch events in SimpleCode and then only examining a small \"snippet\" of time around those candidate rare values. In investigating highly nonlinear systems that may contain bifurcations, the basis for the method would still have to apply. That is to say, there would need to be sufficient data including the different domains of attraction in the training set.\n\nA major crux of this application is whether or not the nonlinear dynamics of such an event would be captured in SimpleCode. SimpleCode does include the most important nonlinear effects in the body-nonlinear hydrostatic and Froude-Krylov forces. The training approach would be relatively straight-forward if SimpleCode was able to qualitatively model these excursions into different dynamical domains.\n\nIf SimpleCode was unable to properly model the bifurcations, there are still approaches that would allow for system identification. In Bury et. al (2023), an application identifying the period-doubling bifurcation in chicken heart rates was performed with a deep-learning classifier involving an LSTM. The approach presented involved system inputs that were not directly related as in the SimpleCode-LAMP case, so it stands to reason that with more qualitative knowledge, a similar approach could succeed in the SimpleCode-LAMP framework.\n\nQuestion 2: One major driver in the success of an LSTM network is the relation between the input and output. In the presented case, the high level of correlation between SimpleCode and LAMP resulted in a network that performed well. However, an LSTM network can still perform well even without a process as correlated to LAMP as SimpleCode. In Levine et.al (2024), ship motion statistics generated in LAMP were\n\naccurately captured by an LSTM approach that only used the incident wave profile as input. In this data-driven approach, the standard deviations of heave, roll, and pitch were well estimated \u2013 most predictions were at least 95% accurate - for various ship speeds, sea states, and relative wave headings.\n\nThat being said, it is unlikely this same approach could be directly applied to the rare event prediction application. In this approach, there must be a way to identify when the large pitch events occur in LAMP. To apply this method to nonlinear processes like extreme bottom slamming loads, an input process that is related and has at least some level of correlation in extremes would need to be identified. One possible candidate for the extreme bottom slamming load response could be relative velocity as predicted by SimpleCode. Of course, further study would need to be performed to investigate the level of correlation and number of events that would have to be considered, as in Figure 7.",
                "similarity": 0.8690547347068787
            }
        ]
    }
}