{
    "Abstract": {
        "cnn1": [
            {
                "text": "Artificial Neural Networks (ANNs) are computational processing systems of which are heavily inspired by way biological nervous systems (such as the human brain) operate. ANNs are mainly comprised of a high number of interconnected computational nodes (referred to as neurons), of which work entwine in a distributed fashion to collectively learn from the input in order to optimise its final output.\n\nThe basic structure of a ANN can be modelled as shown in Figure 1. We would load the input, usually in the form of a multidimensional vector to the input layer of which will distribute it to the hidden layers. The hidden layers will then make decisions from the previous layer and weigh up how a stochastic change within itself detriments or improves the final output, and this is referred to as the process of learning. Having multiple hidden layers stacked upon each-other is commonly called deep learning.\n\n2      Keiron O\u2019Shea et al.\n\nInput Layer    Hidden Layer   Output Layer\n\nInput 1\n\nInput 2\nOutput\n\nInput 3\n\nInput 4\n\nFig. 1: A simple three layered feedforward neural network (FNN), comprised\nof a input layer, a hidden layer and an output layer. This structure is the basis\nof a number of common ANN architectures, included but not limited to Feed-\nforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and\nRecurrent Neural Networks (RNNs).\n\nThe two key learning paradigms in image processing tasks are supervised and\nunsupervised learning. Supervised learning is learning through pre-labelled\ninputs, which act as targets. For each training example there will be a set of\ninput values (vectors) and one or more associated designated output values.\nThe goal of this form of training is to reduce the models overall classification\nerror, through correct calculation of the output value of training example by\ntraining.\n\nUnsupervised learning differs in that the training set does not include any la-\nbels. Success is usually determined by whether the network is able to reduce or\nincrease an associated cost function. However, it is important to note that most\nimage-focused pattern-recognition tasks usually depend on classification using\nsupervised learning.\n\nConvolutional Neural Networks (CNNs) are analogous to traditional ANNs\nin that they are comprised of neurons that self-optimise through learning. Each\nneuron will still receive an input and perform a operation (such as a scalar\nproduct followed by a non-linear function) - the basis of countless ANNs. From\nthe input raw image vectors to the final output of the class score, the entire of\nthe network will still express a single perceptive score function (the weight).\nThe last layer will contain loss functions associated with the classes, and all of\nthe regular tips and tricks developed for traditional ANNs still apply.\n\nThe only notable difference between CNNs and traditional ANNs is that CNNs\nare primarily used in the field of pattern recognition within images. This allows\nus to encode image-specific features into the architecture, making the network\n\nIntroduction to Convolutional Neural Networks    3\n\nmore suited for image-focused tasks - whilst further reducing the parameters required to set up the model.\n\nOne of the largest limitations of traditional forms of ANN is that they tend to struggle with the computational complexity required to compute image data. Common machine learning benchmarking datasets such as the MNIST database of handwritten digits are suitable for most forms of ANN, due to its relatively small image dimensionality of just 28 \u00d7 28. With this dataset a single neuron in the first hidden layer will contain 784 weights (28 \u00d7 28 \u00d7 1 where 1 bare in mind that MNIST is normalised to just black and white values), which is manageable for most forms of ANN.\n\nIf you consider a more substantial coloured image input of 64 \u00d7 64, the number of weights on just a single neuron of the first layer increases substantially to 12,288. Also take into account that to deal with this scale of input, the network will also need to be a lot larger than one used to classify colour-normalised MNIST digits, then you will understand the drawbacks of using such models.",
                "similarity": 0.8390400409698486
            }
        ],
        "cnn3": [
            {
                "text": "CNNs are considered the gold standard in deep learning-based image analysis. For instance, in biomedical imaging, they overcome the drawbacks of subjective analysis in the semi-quantitative visual inspection of samples (Gurcan et al., 2009), and they support experts\n\n\nFigure 6: Class activation maps (CAMs) of a FF trained CNN show which image regions are considered beneficial (yellow) or deleterious (pink) by the network for making its prediction. (A), (C), (E), and (G) display four input images. (B), (D), (F), and (H) are their corresponding CAMs. All examples are from a network with 16 convolutional neurons per layer, filter size 5x5, and trained with a batch size of 50.\n\n| A | B | C | D |\n|---|---|---|---|\n| 1 | Heatmap | 2 | Heatmap |\n| E | F | G | H |\n| 7 | Heatmap | 9 | Heatmap |\n\nColor scale: -0.04 (purple) to 0.04 (yellow)\n\nFigure 7: Class activation maps show that the different layers of the FF-trained CNN provide similar, but yet distinguishable information. (A) shows the CAM obtained from considering both layer 2 and layer 3 together. (B) and (C) show the CAMs obtained respectively only from layer 2 and layer 3.\n\n| A | B | C |\n|---|---|---|\n| Heatmap | Heatmap | Heatmap |\n\nColor scale: -0.04 (purple) to 0.04 (yellow)\n\nduring their daily clinical routine by reducing their workload (Shmatko et al., 2022). Furthermore, their exploitation of the spatial information within images makes them suitable for the deployment of explainable AI tools (such as class activation maps), which highlight the image regions contributing most significantly to the classification outcome. Our implementation of FF trained CNN shows that with the right choice of hyperparameters, this technique is competitive with backpropagation. These results were obtained without implementing all the possible and suggested optimizations such as enforcing symmetry of the loss function (Lee and Song, 2023) or choosing hard, i.e. easily confused, labels for the negative data set, as suggested by Hinton (2022). We propose that our work shows the\n\nSCODELLARO, KULKARNI, ALVES AND SCHR\u00d6TER\n\npotential of FF trained CNNs to address real world computer vision problems. An open question remains if this technique will supersede BP in specific applications. We believe that this potential exists, especially in the cases of neuromorphic hardware and unsupervised learning.\n\nA better understanding of the FF training will however also expand our understanding of the generic concept of neuronal information processing in all its breadth from biological systems to reservoir computing. The demonstrated capability to implement class activation maps offers an initial insight into these research topics. Achieving deeper insights will also mean to understand how the two innovations of FF, providing positive and negative labels and computing a locally defined goodness parameter, contribute to its success individually and synergetically (Tosato et al., 2023). Moreover, a better understanding why it is beneficial to exclude the first layer during the goodness computation (c.f. Appendix B) would be desirable. Subsequent work on FF training should also address its ability to train deeper networks, most likely expanding on the work of Lorberbom et al. (2023). Also the ability of FF training to work with larger and more complex data sets needs to be explored. Finally, its connection to biological neuronal systems (Ororbia and Mali, 2023; Ororbia, 2023) seems a promising research direction.",
                "similarity": 0.807677686214447
            }
        ],
        "cnn5": [
            {
                "text": "Image classification is a core task in computer vision and machine learning, where the goal is to assign one or more labels to an image based on its content. It serves as the foundation for numerous real-world applications, such as facial recognition, autonomous driving, medical diagnosis, and smart manufacturing [1], [2]. The CIFAR-10 dataset [3], introduced by Krizhevsky and Hinton [3], has become a standard benchmark in this field. It consists of 60,000 color images divided into 10 distinct classes, each representing a unique object category, such as airplanes, automobiles, birds, and cats. The dataset is balanced, with an equal number of samples for each class, making it an ideal testbed for evaluating classification algorithms. The relatively small size of the dataset allows for efficient experimentation while retaining sufficient complexity to challenge advanced models.\n\nConvolutional Neural Networks (CNNs) have been the backbone of most state-of-the-art models in image classification. CNNs excel in extracting hierarchical spatial features from images, enabling them to capture low-level edges and textures in earlier layers and high-level semantic features in deeper layers. This hierarchical feature extraction capability, combined with their translational invariance, makes CNNs particularly well-suited for visual tasks. However, achieving high accuracy on the CIFAR-10 dataset presents several challenges. First, the dataset's small image resolution of 32x32x3 limits the amount of information and detail that can be extracted, thereby constraining the model's capacity to distinguish between similar classes. Second, the limited size of the dataset increases the risk of overfitting, especially for deeper networks with a large number of parameters. This necessitates the use of effective regularization techniques and data augmentation strategies. Finally, there is a need for a robust network design that strikes a balance between depth, parameter efficiency, and regularization to achieve both high accuracy and generalization.\n\nThis paper aims to address these challenges by proposing an enhanced CNN architecture specifically designed for CIFAR-10 image classification. As suggested in [4], by integrating deeper convolutional blocks, batch normalization to stabilize training, and dropout to mitigate overfitting, the proposed model effectively extracts rich hierarchical features while maintaining robustness. Experimental results demonstrate that this architecture achieves superior performance, highlighting its potential for tackling small-scale yet complex image classification tasks.\n\nIn this paper, we address these challenges by proposing an enhanced CNN architecture with deeper convolutional layers, batch normalization for stable training, and dropout regularization to mitigate overfitting. Our model surpasses standard CNN baselines in accuracy, highlighting the importance of architectural refinement in deep learning applications.",
                "similarity": 0.8322737812995911
            }
        ],
        "pdf2": [
            {
                "text": "Recent years have witnessed the advent of deep learning methods for structural biology culminating in the award of the Nobel Prize in Chemistry. AlphaFold (Jumper et al., 2021) revolutionized protein structure prediction, equipping the field with millions of new structures. Breakthroughs go beyond structure prediction, notably in protein design (Watson et al., 2023; Dauparas et al., 2022), drug discovery (Schneuing et al., 2024; Corso et al., 2022) or fundamental biology (van Kempen et al., 2022). While it is tempting to attribute the success of these methods to the increase in available structural data caused by AlphaFold, most of the methods are actually not reliant on them. Instead, it seems that these breakthroughs result from progress in training neural encoders that directly model protein structures (Jing et al., 2020; Zhang et al., 2022b; Gainza et al., 2020; Wang et al., 2022). This progress is in turn rooted in solid competitions (CASP, CAPRI), and benchmarks (Townshend et al., 2021a; Kucera et al., 2023; Zhu et al., 2022; Jamasb et al., 2024; Notin et al., 2023). By setting clear goals, such benchmarks are the foundation for the development of structure encoders. Yet to date, structure-function benchmarks have focused on proteins.\n\nRibonucleic acids (RNAs) are a large family of molecules which support biological functions along every branch of the tree of life. Besides messenger RNAs, non-coding RNAs carry out biological functions by adopting complex 3D folds (Cech & Steitz, 2014) like proteins do and take up diverse roles in cellular functions, including gene regulation, RNA processing, and protein synthesis (Statello et al., 2021). However, our understanding of non-coding RNAs and their functions remains limited. This can be largely attributed to the negatively charged nature of RNA backbones, which makes it flexible and limits the availability of high-resolution RNA structures, and imposes significant modeling challenges. Another predominant challenge to a functional understanding of RNA 3D structure lies in the lack of infrastructure for the development and evaluation of function prediction models. In this work, we propose a benchmarking suite to act as this facilitating framework.\n\nOur key contributions include:\n\n- Seven tasks related to RNA 3D structure that represent various biological challenges. Each task consists of a dataset, a splitting strategy, and an evaluation method, laying the ground for comparable, reproducible model development.\n\n- End-to-end reproducible and modular access to task data. Modular annotators, filters and splitting strategies, both novel and from existing literature, facilitate the addition of new tasks by other researchers across fields.\n\n*Equal contribution \u2020Equal supervision 1Max Planck Institute of Biochemistry, Munich, Germany 2Mines Paris, PSL Research University, CBIO, Paris, France 3Vanderbilt University, Nashville, Tennessee, USA. Correspondence to: Luis Wyss <wyss@biochem.mpg.de>.\n\nA preprint.\n\nBenchmark for RNA 3D Structure Modeling",
                "similarity": 0.8272076845169067
            }
        ],
        "pdf4": [
            {
                "text": "Balasubramanian, K., Fan, J. and Yang, Z. (2018). Tensor methods for additive index models under discordance and heterogeneity. arXiv preprint arXiv:1807.06693 .\n\nBauer, B. and Kohler, M. (2019). On deep learning as a remedy for the curse of dimensionality in nonparametric regression. The Annals of Statistics 47 2261\u20132285.\n\nBauer, F., Pereverzev, S. and Rosasco, L. (2007). On regularization algorithms in learning theory. Journal of complexity 23 52\u201372.\n\nBreymann, W. and L\u00fcthi, D. (2013). ghyp: A package on generalized hyperbolic distributions. Manual for R Package ghyp .\n\nCandes, E. J., Li, X., Ma, Y. and Wright, J. (2009). Robust principal component analysis? arXiv preprint arXiv: 0912.3599 .\n\nChangliang Zou, Y. K. and Zhang, W. (2022). Estimation of low rank high-dimensional multivariate linear models for multi-response data. Journal of the American Statistical Association 117 693\u2013703.\n\nChen, K., Dong, H. and Chan, K.-S. (2012). Reduced rank regression via adaptive nuclear norm penalization. arXiv preprint arXiv:1201.0381 .\n\nChen, X., Zou, C. and Cook, R. D. (2010). Coordinate-independent sparse sufficient dimension reduction and variable selection. The Annals of Statistics 38 3696 \u2013 3723.\n\nDamian, A., Lee, J. and Soltanolkotabi, M. (2022). Neural networks can learn representations with gradient descent. In Conference on Learning Theory.\n\nFriedman, J. H. and Stuetzle, W. (1981). Projection pursuit regression. Journal of the American Statistical Association 76 817\u2013823.\n\nHui Zou, T. H. and Tibshirani, R. (2006). Sparse principal component analysis. Journal of Computational and Graphical Statistics 15 265\u2013286.\n\nHyv\u00e4rinen, A. and Dayan, P. (2005). Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research 6 695\u2013709.\n\nJanzamin, M., Sedghi, H. and Anandkumar, A. (2014). Score function features for discriminative learning: Matrix and tensor framework. arXiv preprint arXiv:1412.2863 .\n\nKobak, D., Bernaerts, Y., Weis, M. A., Scala, F., Tolias, A. S. and Berens, P. (2021). Sparse reduced-rank regression for exploratory visualisation of paired multivariate data. Journal of the Royal Statistical Society Series C: Applied Statistics 70 980\u20131000.\n\nLee, W. and Liu, Y. (2012). Simultaneous multiple response regression and inverse covariance matrix estimation via penalized gaussian maximum likelihood. Journal of Multivariate Analysis 111 241\u2013255.\n\nLi, K.-C. (1991). Sliced inverse regression for dimension reduction. Journal of the American Statistical Association 86 316\u2013327.\n\nLi, K.-C. (1992). On principal hessian directions for data visualization and dimension reduction: Another application of stein's lemma. Journal of the American Statistical Association 87 1025\u20131039.\n\nLi, K.-C. and Duan, N. (1989). Regression analysis under link violation. The Annals of Statistics 1009\u20131052.\n\nLi, Y. and Turner, R. E. (2017). Gradient estimators for implicit models. arXiv preprint arXiv:1705.07107 .\n\nLu, Z., Monteiro, R. D. and Yuan, M. (2012). Convex optimization methods for dimension reduction and coefficient estimation in multivariate linear regression. Mathematical Programming 131 163\u2013194.\n\nMakhzani, A. and Frey, B. (2013). K-sparse autoencoders. arXiv preprint arXiv:1312.5663 .\n\nMeng, C., Song, Y., Li, W. and Ermon, S. (2021). Estimating high order gradients of the data distribution by denoising. Advances in Neural Information Processing Systems 34 25359\u201325369.\n\nMousavi-Hosseini, A., Park, S., Girotti, M., Mitliagkas, I. and Erdogdu, M. A. (2022).\nNeural networks efficiently learn low-dimensional representations with sgd. arXiv preprint\narXiv:2209.14863 .\n\nMukherjee, A. and Zhu, J. (2011). Reduced rank ridge regression and its kernel extensions.\nStatistical analysis and data mining: the ASA data science journal 4 612\u2013622.\n\nO'Rourke, S., Vu, V. and Wang, K. (2018). Random perturbation of low rank matrices:\nImproving classical bounds. Linear Algebra and its Applications 540 26\u201359.\n\nPearson, K. (1901). Liii. on lines and planes of closest fit to systems of points in space. The\nLondon, Edinburgh, and Dublin philosophical magazine and journal of science 2 559\u2013572.\n\nRifai, S., Vincent, P., Muller, X., Glorot, X. and Bengio, Y. (2011). Contractive auto-\nencoders: Explicit invariance during feature extraction. In Proceedings of the 28th international\nconference on international conference on machine learning.\n\nScala, F., Kobak, D., Bernabucci, M., Bernaerts, Y., Cadwell, C. R., Castro, J. R.,\nHartmanis, L., Jiang, X., Laturnus, S., Miranda, E. et al. (2021). Phenotypic variation\nof transcriptomic cell types in mouse motor cortex. Nature 598 144\u2013150.\n\nShi, J., Sun, S. and Zhu, J. (2018). A spectral approach to gradient estimation for implicit\ndistributions. In International Conference on Machine Learning.\n\nSimon, N., Friedman, J. and Hastie, T. (2013). A blockwise descent algorithm for group-\npenalized multiresponse and multinomial regression. arXiv preprint arXiv:1311.6529 .\n\nSong, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of the data\ndistribution. Advances in neural information processing systems 32.\n\nSong, Y., Garg, S., Shi, J. and Ermon, S. (2020). Sliced score matching: A scalable approach\nto density and score estimation. In Uncertainty in Artificial Intelligence.\n\nStrathmann, H., Sejdinovic, D., Livingstone, S., Szabo, Z. and Gretton, A. (2015).\nGradient-free hamiltonian monte carlo with efficient kernel exponential families. Advances in\nNeural Information Processing Systems 28 955\u2013963.\n\nTan, K. M., Wang, Z., Zhang, T., Liu, H. and Cook, R. D. (2018). A convex formulation for\nhigh-dimensional sparse sliced inverse regression. Biometrika 105 769\u2013782.\n\nVershynin, R. (2018a). High-dimensional probability: An introduction with applications in data\nscience, vol. 47. Cambridge university press.\n\nVershynin, R. (2018b). High-dimensional probability: An introduction with applications in data\nscience, vol. 47. Cambridge university press.\n\nVincent, P. (2011). A connection between score matching and denoising autoencoders. Neural\ncomputation 23 1661\u20131674.\n\nVincent, P., Larochelle, H., Bengio, Y. and Manzagol, P.-A. (2008). Extracting and\ncomposing robust features with denoising autoencoders. In Proceedings of the 25th international\nconference on Machine learning.\n\nWANG, W., LIANG, Y. and XING, E. (2013). Block regularized lasso for multivariate multi-response linear regression. In Artificial intelligence and statistics.\n\nXU, X. (2020). On the perturbation of the moore\u2013penrose inverse of a matrix. Applied Mathematics and Computation 374 124920.\n\nYANG, Z., BALASUBRAMANIAN, K. and LIU, H. (2017a). High-dimensional non-Gaussian single index models via thresholded score function estimation. In Proceedings of the 34th International Conference on Machine Learning, vol. 70.\n\nYANG, Z., BALASUBRAMANIAN, K., WANG, Z. and LIU, H. (2017b). Learning non-gaussian multi-index model via second-order stein's method. Advances in Neural Information Processing Systems 30 6097\u20136106.\n\nYU, Y., WANG, T. and SAMWORTH, R. J. (2015). A useful variant of the davis\u2013kahan theorem for statisticians. Biometrika 102 315\u2013323.\n\nYUAN, M., EKICI, A., LU, Z. and MONTEIRO, R. (2007). Dimension reduction and coefficient estimation in multivariate linear regression. Journal of the Royal Statistical Society Series B: Statistical Methodology 69 329\u2013346.\n\nZHOU, Y., SHI, J. and ZHU, J. (2020). Nonparametric score estimators. In International Conference on Machine Learning.\n\n17\n\nSupplementary Material for\n\"Nonlinear Multiple Response Regression and Learning of Latent Spaces\"\n\nYe Tian, Sanyou Wu and Long Feng",
                "similarity": 0.8253905773162842
            }
        ],
        "cnn2": [
            {
                "text": "",
                "similarity": 0.8390400409698486
            }
        ],
        "cnn4": [
            {
                "text": "Convolutional Neural Networks (CNNs) have revolutionized the field of deep learning, especially in processing grid-like data structures such as images [1]. Their effectiveness in tasks like image classification [2, 3], object detection [4, 5], semantic segmentation [6, 7] and image generation [8] stem from their ability to effectively learn spatial features. Convolutional layers, using filters or kernels, capture local patterns and extract features from input images. One important feature of convolutional layers is the shared weights implemented by kernels. This allows for efficient deep learning on images, as using only fully connected layers for such tasks would result in unfathomable numbers of parameters. Pooling layers, like max pooling and average pooling, reduce the spatial dimensions of these features, helping the network to focus on the most significant aspects.\n\nDespite their popularity, CNNs face challenges in computational efficiency and adaptability. There have been several convolutional neural network architectures that have been proposed that are aimed at efficiency. Some of such architectures include MobileNet [12] and EfficientNet [13]. However, such traditional CNNs, with fixed architectures and number of parameters, may not perform uniformly across different types of input data with varying levels of complexity.\n\nNeural Architecture Search (NAS), a method for selecting optimal neural network architectures, has been a response to this challenge. NAS aims to obtain the best model for a specific task under certain constraints [14]. However, NAS is often resource-intensive due to the need to train multiple candidate models. in order to determine the optimal architecture. It is estimated that the carbon emission produced when using NAS to train a transformer model can amount to five times the lifetime carbon emissions of an average car [15]. This highlights the importance of finding suitable\n\narchitectures for neural networks, yet also points to the limitations of current approaches in terms of static structure and proneness to over-parameterization.\n\nSelf Expanding Neural Networks (SENN), introduced in [9], offer a promising direction. Inspired by neurogenesis, SENN dynamically adds neurons and fully connected layers to the architecture during training using a natural expansion score (defined in section 2.1) as a criteria to guide this process. This helps overcome the problem of over-parametrization. However, its application has been limited to multilayer perceptrons, with extensions to more practical architectures like CNNs identified as a future research prospect.\n\nOur study aims to develop a Self Expanding Convolutional Neural Network (SECNN), building on the concept of SENN and applying it to modern vision tasks. To the best of our knowledge, there has been no research on Self Expanding CNNs, despite the potential they hold for addressing model efficiency and adaptability in vision tasks. Unlike existing approaches that often require restarting training after modifications or rely on preset mechanisms for expansion, our approach utilizes the natural expansion score for dynamic and optimal model expansion. This research represents a significant step in developing adaptable, efficient CNN models for a variety of vision-related tasks.\n\nThe contributions of this research are as follows:\n\n- Developing a Self Expanding CNN that dynamically determines the optimal model size based on the task, thereby enhancing efficiency.\n- Eliminating the need to train multiple CNN models of varying sizes by allowing for the extraction of checkpoints at diverse complexity levels.\n- Eliminating the need to restart the training process after expanding the CNN model.",
                "similarity": 0.8390400409698486
            }
        ],
        "pdf1": [
            {
                "text": "Autonomous underwater vehicles (AUVs) are valuable tools for exploring and operating underwater. They are used in a wide range of applications, including seafloor mapping, underwater construction and inspection, environmental monitoring, and studying marine life [1], [2]. To accomplish their task accurately and robustly, precise navigation is required. Commonly, this is achieved by fusing inertial sensors with a Doppler velocity log (DVL) [3]. The DVL is an acoustic sensor that utilizes the Doppler effect. This device transmits four acoustic beams to the seafloor, which are then reflected back. Based on the frequency shift, the DVL calculates the velocity of each beam and then estimates the velocity vector of the AUV [4].\n\nData-driven methods have been employed in AUV-related tasks with promising outcomes [5]\u2013[9]. DCNet, a data-driven framework that utilizes a two-dimensional convolution kernel in an innovative way, demonstrated its ability to improve the process of DVL calibration [10]. Deep-learning frameworks have been used to estimate real-world scenarios of missing DVL beams in partial and complete outage scenarios [11], [12]. Additionally, when all beams are available, the BeamsNet approach [13] offers a more accurate and robust velocity solution using a dedicated deep-learning framework. Deep learning methods were suggested for the fusion process to adaptively estimate the process noise covariance in the inertial DVL fusion process [14], [15]. Recently, an end-to-end deep learning approach was suggested to estimate the AUV acceleration vector, which is introduced to the navigation filter as an additional measurement [16]. In normal operating conditions of the DVL, the BeamsNet approach outperforms model-based approaches. This method employs both inertial measurements and past DVL measurements to estimate the current velocity vector. When updating the navigation filter with this measurement, a cross-correlation arises between the BeamsNet velocity vector measurement and the inertial-based process noise. This process-measurement cross-correlation matrix should be taken into account in the navigation filter to obtain a desired matched filter [17].\n\nIn this paper, we employ the inertial/DVL navigation filter based on the extended Kalman filter (EKF), taking into account the process-measurement cross-correlation matrix. The latter is calculated using the inertial and BeamsNet error sources. Using two real-world underwater AUV datasets, we show the necessity of the cross-correlation matrix to allow filter consistency and robustness.\n\nThe rest of the paper is organized as follows: Section II formulates the problem and presents the theoretical foundation for incorporating cross-correlations within the extended Kalman filter. Section III introduces the proposed cross-correlation-aware deep INS/DVL fusion framework, detailing the integration of BeamsNet with a modified filter formulation. Section IV presents the experimental results and performance analysis based on real-world AUV trajectories. Finally, Section V concludes the findings in the paper.\n\n*Corresponding author: N. Cohen (email: ncohe140@campus.haifa.ac.il).\n\nII. PROBLEM FORMULATION\n\nA. EKF with Correlated Noise\n\nIn the classical derivation of the error-state EKF, it is typically assumed that the process noise and measurement noise are uncorrelated. This is due to the fact that the different sensors provide the information for the process and update. However, in some cases, like the one we introduce in this paper, non-negligible cross-correlations may exist between the process and measurement noise terms. In this section, we present an error-state EKF framework that accounts for such correlation, following the modified Kalman filter equations that explicitly incorporate this dependency.\n\nLet the system dynamics and measurement model be described by the discrete-time equations [18], [19]:\n\n$$x_k = F_{k-1}x_{k-1} + G_{k-1}w_{k-1},$$\n\nwhere $x_k$ denotes the system state vector at time step $k$, $F_{k-1}$ is the state transition matrix that propagates the state from time $k - 1$ to $k$, and $G_{k-1}$ is the process noise input matrix.\n\nThe measurement model is:\n\n$$y_k = H_kx_k + v_k,$$\n\nwhere, $y_k$ is the measurement vector at time step $k$ and $H_k$ is the observation matrix that maps the state vector to the measurement space. Additionally, $w_k \\sim N(0, Q_k)$ denotes the process noise with associated process noise covariance $Q_k$, and $v_k \\sim N(0, R_k)$ denotes the measurement noise with associated measurement noise covariance $R_k$.\n\nThe cross-correlation matrix between the process and measurement noise covariances is defined as [17]:\n\n$$E[w_kv_j^T] = M_k\\delta_{k-j+1},$$\n\nindicating that the process noise at time $k$ is correlated with the measurement noise at time $k+1$. This structure arises naturally in systems where the same external disturbance influences both the system dynamics and the measurement process, albeit with a one-step time lag.\n\nTo incorporate this cross-correlation into the Kalman gain computation, the innovation covariance must be adjusted such that the Kalman gain becomes:\n\n$$K_k = (P_k^-H_k^T + M_k) \\cdot (H_kP_k^-H_k^T + H_kM_k + M_k^TH_k^T + R_k)^{-1},$$\n\nwhere $P_k^-$ is the prior error covariance matrix.\n\nConsequently, the posterior error covariance is updated according to:\n\n$$P_k = P_k^- - K_k(H_kP_k^- + M_k^T).$$\n\nThese modified expressions account for the non-zero cross-correlation between process and measurement noise, improving filter consistency in scenarios where this assumption is violated. The rest of the error state EKF equations and process remain the same with the exception of the Kalman gain (4) and can be seen in [20], for example.\n\nB. Cross-Correlation within INS/DVL Fusion\n\nRecent advances in deep learning have demonstrated significant potential in time series estimation and sensor fusion, especially in navigation systems where standard model-based filters often struggle with drift, nonlinearity, or degraded measurement conditions. By leveraging the expressive power of deep neural networks (DNNs), it is possible to model complex dependencies between sensor modalities and capture higher-order temporal patterns in the data. In particular, DNN-based approaches that jointly process inertial and acoustic measurements can yield more accurate velocity estimates than classical extended Kalman filtering alone.\n\nConsider a system where the goal is to estimate the vehicle's velocity using both the inertial sensors, which include accelerometers that provide the specific force vector $f_k \\in \\mathbb{R}^3$ and gyroscopes that measure the angular velocity vector $\\omega_k \\in \\mathbb{R}^3$, and a DVL, which provides beam velocity measurements $z_k^{DVL} \\in \\mathbb{R}^4$. A deep neural network can be trained to produce a fused velocity estimate via a nonlinear function:\n\n$$\\hat{v}_k = \\mathcal{F}_\\theta(f_{k-T:k}, \\omega_{k-T:k}, z_{k-T:k}^{DVL}),$$\n\nwhere $\\mathcal{F}_\\theta(\\cdot)$ denotes the DNN with parameters $\\theta$ and the inputs consist of a temporal window of size $T$. The output $\\hat{v}_k$ is the estimated velocity vector at time $k$, typically expressed in the body frame.\n\nThe challenge arises from the stochastic properties of the inputs. The IMU measurements $f_k$ and $\\omega_k$ are driven by process noise $w_k$, while the DVL beams $z_k^{DVL}$ are corrupted by measurement noise $v_k$. Let us denote:\n\n$$f_k = f_k^{true} + w_k^f$$\n$$\\omega_k = \\omega_k^{true} + w_k^\\omega$$\n$$z_k^{DVL} = z_k^{true} + v_k$$\n\nwhere $w_k^f$ and $w_k^\\omega$ represent the accelerometer and gyroscopes process noise, respectively, and $v_k$ denotes the DVL measurement noise. Due to the nonlinear mapping, $\\mathcal{F}_\\theta(\\cdot)$, the output velocity estimate becomes a complex function of all the noise sources:\n\n$$\\hat{v}_k = \\mathcal{F}_\\theta(f_k^{true} + w_k^f, \\omega_k^{true} + w_k^\\omega, z_k^{true} + v_k).$$\n\nUnlike linear estimators, where uncorrelated inputs lead to uncorrelated outputs, the nonlinear dependency structure in $\\mathcal{F}_\\theta$ causes interactions between $w_k$ and $v_k$. As a result, the effective measurement used for state correction, the output of the DNN, embeds cross-correlations between the process and measurement noise. That is,\n\n$$E[w_kv_k^T] \\neq 0,$$\n\nand the distribution of the estimation error becomes analytically intractable due to the black-box nature of the network. Therefore, when such a fused estimate is used as an update measurement in an error-state EKF, the assumption of noise independence no longer holds. This violates the foundational\n\n\n```mermaid\ngraph LR\nDVL[DVL] --> BeamsNet\nInertial[Inertial Sensors] --> BeamsNet\nBeamsNet --> CrossCorrelation[Cross-Correlation]\nDVL --> EKF\nInertial --> EKF\nCrossCorrelation --> EKF\nEKF --> Position\nEKF --> Velocity\nEKF --> Orientation\n```\n\nFig. 1: Our proposed approach block diagram illustrates how inertial and DVL measurements are utilized as inputs to the BeamsNet framework. This model generates an enhanced velocity vector measurement update, which is then passed, together with the inertial uncertainty, to the cross-correlation block. This block computes the cross-correlation matrix, which is subsequently integrated into the EKF to derive the navigation solution.\n\nassumptions of standard Kalman filtering theory and necessitates either reformulation of the filter to incorporate the cross-covariance or empirical techniques to mitigate its effect.",
                "similarity": 0.8322737812995911
            }
        ],
        "pdf3": [
            {
                "text": "",
                "similarity": 0.8111008405685425
            }
        ]
    }
}