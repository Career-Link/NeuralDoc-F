# Overview of Project:

First input pdf

convert into markdown file (process known as parsing)(text or image)(PyMU PDF)

convert it into json

store it into Milvus vector database (used for searching and managing large amounts of unstructured data like images, videos, and text. Instead of storing data in rows and columns like a traditional database, Milvus stores vectors, which are numerical representations of data.)

User query dalega

This is converted into embeddings and then Similarity search is performed

Then through LLM what data we got, convert it into markdown

this markdown is converted into LATEX format 

FINALLY, WE GET THE RESULT PDF...
_____________________________________________________________________________

go to llama cloud

parse me jake pdf parse kra k dekh lo...markdown me result ayega...kyuki heading vgaira easily identify kr skte h...(frameworks like langchain)

generated key through llamaparse: llx-9HPAGu3yChXXHtwsV9fIJ62ggnKyjPKthPkJCdn0dCzGSapu

open folder in VSCode and then open terminal:
python -m venv myenv
source/Scripts/activate
pip install python-dotenv llama-parse llama-index
python parse.py
______________________________________________

Other way is to not publicly show your api key in parse.py python file directly

Make a file named .env and write LLAMA_CLOUD_API_KEY='llx-9HPAGu3yChXXHtwsV9fIJ62ggnKyjPKthPkJCdn0dCzGSapu'

and remove { , api_key=''} part from parser object...and now load_dotenv() will automatically fetch that entered thing in .env file...

run through python parse.py
______________________________________________

copy 7 march file into 5th march folder because there we have our markdown files and all

then open docker

then: docker compose up -d (Starts Milvus using Docker)

then activate environment: myenv\Scripts\activate

pip install pymilvus (install python client for Milvus)

python
>>> from pymilvus import MilvusClient
>>> client=MilvusClient(uri="http://localhost:19530",token="root:Milvus")
>>> client.create_collection("test",dimension=5) (Creates a collection named "test" with 5D vectors.)

>>> client.list_collections() (Lists all collections)
>>> from pymilvus import utility
>>> from pymilvus import connections
>>> connections.connect(uri="http://localhost:19530", token="root:Milvus") (MilvusClient connects to Milvus at http://localhost:19530)
>>> utility.drop_collection('test') (Deletes the "test" collection)
>>> client.list_collections()  
[]
>>> 

python parser.py cnn1.pdf
___________________________________________________________________________________________________________________________________

made changes in parser.py to test function of function -> _parse_pdf_to_markdown()

it gives two outputs one document, and other image_with_caption (an extracted image from the PDF along with its associated caption)

ek hi image extract hui h (mtlb vo particular pagewidth k dimension me jo suit ho rhi thi vhi extract hui h...)

parser._parse_markdown_to_json(document) krne se key value me ans aayega

___________________________________________________________________________________________________________________________________

Samba server IP: 192.168.43.103
Schema delete hone k bad...sir ne docker ki sari cheeze delete ki...volumes delete krI...

docker compose up -d
python retrieval.py

do output aye h...keywordbased.txt me pdf ka naam ayega...
default me kuch basic headings like introduction, abstract vgaira me search krega...

Sentence transformer: a model for embeddings generation

Vector dimension(1024) jitna jada...utna processing speed and accuracy k sath similarity search krega...

Vector database: IP search (Inner Product), Euclidean distance search, cosine search
Threshold: 0.85
__________________________________________________________________________________________________________

pip install google.generativeai
then generate a API Key from google ai studio: Create and paste in .env file with GEMINI_API_KEY variable name...


[install MikTeX ----> a package that helps turn LaTeX code (in .tex files) into PDFs]
[and it further process this .tex file and convert into a good looking PDF]

[.tex file is a LaTeX document for creating a structured research paper or review article]
[.tex file automation.py ko chla k ayi... jisme ToLatex.py ka reference h and isi se .tex file ayi]

[So, The automation.py has referencing of ToLatex.py inside itself... 
Hence, when automation.py is run, a .tex file is generated and this .tex file is processed and converted into a PDF via MikTex...]


python automation.py dump data/cnn2.pdf data/cnn3.pdf data/cnn4.pdf data/cnn5.pdf output
python automation.py search "Neural Networks"  
___________________________________________________________________________________________________________________________________
----> docker build -t steamlit-app . (docker image creation)
(1) It packs your app and all its dependencies into a package. 
(2) This means your app will run the same way on different machine specifications.
(3) It avoids those “it works on my machine” issues.
______________________________________________________________________________________

----> "docker run -p 8501:8501 streamlit_app"

# Step-by-Step Example:
(1) Streamlit App Inside Docker: Runs on port 8501 (like Door #8501 inside the house).

(2) Now in the Docker Command: "docker run -p 8501:8501 streamlit-app", 
    the -p 8501:8501 = "Connect my laptop’s Door 8501 to the container’s Door 8501."

(3) Browser Access: Go to http://localhost:8501 → Traffic goes through your laptop’s Door 8501 → Reaches the app inside Docker.

# Key Points:
(1) 0.0.0.0: Means "let anyone outside the container (like your laptop) connect."
(2) Why "localhost:8501" works: Your laptop’s door (8501) is linked to the container’s door (8501).
___________________________________________________________________________________________________________________________________

go to huggingface.co...
make credentials and save in doc file...

# python-dotenv
# llama-parse
# llama-index
# pymilvus
# sentence-transformers
# PyMuPDF
# dotenv
# pymilvus
# torch
# aiohappyeyeballs==2.4.6
# aiohttp==3.11.13
# aiosignal==1.3.2
# altair==5.5.0
# annotated-types==0.7.0
# anyio==4.8.0
# attrs==25.1.0
# blinker==1.9.0
# cachetools==5.5.2
# certifi==2025.1.31
# charset-normalizer==3.4.1
# click==8.1.8
# dataclasses-json==0.6.7
# Deprecated==1.2.18
# dirtyjson==1.0.8
# dotenv==0.9.9
# filelock==3.17.0
# filetype==1.2.0
# frozenlist==1.5.0
# fsspec==2025.2.0
# gitdb==4.0.12
# GitPython==3.1.44
# google-ai-generativelanguage==0.6.15
# google-api-core==2.24.1
# google-api-python-client==2.162.0
# google-auth==2.38.0
# google-auth-httplib2==0.2.0
# google-generativeai==0.8.4
# googleapis-common-protos==1.68.0
# greenlet==3.1.1
# grpcio==1.67.1
# grpcio-status==1.67.1
# h11==0.14.0
# httpcore==1.0.7
# httplib2==0.22.0
# httpx==0.28.1
# huggingface-hub==0.29.1
# idna==3.10
# Jinja2==3.1.5
# joblib==1.4.2
# jsonschema==4.23.0
# jsonschema-specifications==2024.10.1
# llama-cloud==0.1.13
# llama-cloud-services==0.6.3
# llama-index-core==0.12.22
# llama-parse==0.6.2
# markdown-it-py==3.0.0
# MarkupSafe==3.0.2
# marshmallow==3.26.1
# mdurl==0.1.2
# # milvus-lite==2.4.11 
# mpmath==1.3.0
# multidict==6.1.0
# mypy-extensions==1.0.0
# narwhals==1.28.0
# nest-asyncio==1.6.0
# networkx==3.4.2
# nltk==3.9.1
# numpy==2.2.3
# # nvidia-cublas-cu12==12.4.5.8
# # nvidia-cuda-cupti-cu12==12.4.127
# # nvidia-cuda-nvrtc-cu12==12.4.127
# # nvidia-cuda-runtime-cu12==12.4.127
# # nvidia-cudnn-cu12==9.1.0.70
# # nvidia-cufft-cu12==11.2.1.3
# # nvidia-curand-cu12==10.3.5.147
# # nvidia-cusolver-cu12==11.6.1.9
# # nvidia-cusparse-cu12==12.3.1.170
# # nvidia-cusparselt-cu12==0.6.2
# # nvidia-nccl-cu12==2.21.5
# # nvidia-nvjitlink-cu12==12.4.127
# # nvidia-nvtx-cu12==12.4.127
# packaging==24.2
# pandas==2.2.3
# pillow==11.1.0
# propcache==0.3.0
# proto-plus==1.26.0
# protobuf==5.29.3
# pyarrow==19.0.1
# pyasn1==0.6.1
# pyasn1_modules==0.4.1
# pydantic==2.10.6
# pydantic_core==2.27.2
# pydeck==0.9.1
# Pygments==2.19.1
# pymilvus==2.5.4
# PyMuPDF==1.25.3
# pyparsing==3.2.1
# python-dateutil==2.9.0.post0
# python-dotenv==1.0.1
# pytz==2025.1
# PyYAML==6.0.2
# referencing==0.36.2
# regex==2024.11.6
# requests==2.32.3
# rich==13.9.4
# rpds-py==0.23.1
# rsa==4.9
# safetensors==0.5.3
# scikit-learn==1.6.1
# scipy==1.15.2
# sentence-transformers==3.4.1
# setuptools==75.8.2
# six==1.17.0
# smmap==5.0.2
# sniffio==1.3.1
# SQLAlchemy==2.0.38
# streamlit==1.42.2
# sympy==1.13.1
# tenacity==9.0.0
# threadpoolctl==3.5.0
# tiktoken==0.9.0
# tokenizers==0.21.0
# toml==0.10.2
# torch==2.6.0
# tornado==6.4.2
# tqdm==4.67.1
# transformers==4.49.0
# # triton==3.2.0  
# typing-inspect==0.9.0
# typing_extensions==4.12.2
# tzdata==2025.1
# ujson==5.10.0
# uritemplate==4.1.1
# urllib3==2.3.0
# watchdog==6.0.0
# wrapt==1.17.2
# yarl==1.18.3









































# FROM python:3.10-slim-buster
# WORKDIR /volumes
# COPY requirements.txt requirements.txt
# RUN pip install -r requirements.txt
# RUN apt install pandoc
# RUN apt install texlive-latex-base textlive-latex-extra textlive-fonts-recommended

# COPY . .

# EXPOSE 8501

# CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]


# FROM python:3.10-slim-buster

# # Explicitly set WORKDIR to /volumes
# WORKDIR /volumes

# # Install system dependencies with cleanup
# RUN apt-get update && \
#     apt-get install -y --no-install-recommends \
#     pandoc \
#     texlive-latex-base \
#     texlive-latex-extra \
#     texlive-fonts-recommended && \
#     rm -rf /var/lib/apt/lists/*

# # Install Python dependencies
# COPY requirements.txt .
# RUN pip install --no-cache-dir --upgrade pip && \
#     pip install --no-cache-dir pymilvus==2.5.4 grpcio==1.67.1 grpcio-status==1.67.1 && \
#     pip install --no-cache-dir -r requirements.txt

# # Copy application code
# COPY . .

# EXPOSE 8501
# CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]


# FROM python:3.10-slim-buster
# WORKDIR /volumes
# RUN apt-get update && \
#     apt-get install -y --no-install-recommends \
#     pandoc \
#     texlive-latex-base \
#     texlive-latex-extra \
#     texlive-fonts-recommended && \
#     rm -rf /var/lib/apt/lists/*

# COPY requirements.txt .
# RUN pip install --no-cache-dir --upgrade pip && \
#     pip install --no-cache-dir --no-deps -r requirements.txt

# COPY . .

# EXPOSE 8501
# CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]